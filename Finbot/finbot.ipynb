{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Install Dependencies\n",
    "!pip install -q transformers datasets peft accelerate pandas tqdm bitsandbytes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Cell 2: Import Libraries\n",
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    BitsAndBytesConfig\n",
    ")\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    get_peft_model,\n",
    "    TaskType,\n",
    "    prepare_model_for_kbit_training\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Cell 3: Set up Environment\n",
    "# Use Llama 3 model instead of TinyLlama\n",
    "model_id = \"meta-llama/Meta-Llama-3-8B\"  # You can change this to other Llama 3 variants\n",
    "output_dir = \"./llama3-lora-finetuned\"\n",
    "\n",
    "# Create output directory\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Check for GPU\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n",
      "Loaded 152 training examples and 17 test examples\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Cell 4: Load Dataset\n",
    "def load_dataset():\n",
    "    print(\"Loading dataset...\")\n",
    "    df = pd.read_csv(\"Dataset.csv\")\n",
    "    \n",
    "    # Updated prompt template to match Llama 3 chat format\n",
    "    df[\"text\"] = df.apply(\n",
    "        lambda row: f\"<|im_start|>user\\n{row['Question']}<|im_end|>\\n<|im_start|>assistant\\n{row['Answer']}<|im_end|>\", \n",
    "        axis=1\n",
    "    )\n",
    "    \n",
    "    # Split into train and test\n",
    "    train_df = df.sample(frac=0.9, random_state=42)\n",
    "    test_df = df.drop(train_df.index)\n",
    "    \n",
    "    # Convert to Hugging Face datasets\n",
    "    train_dataset = Dataset.from_pandas(train_df[[\"text\"]])\n",
    "    test_dataset = Dataset.from_pandas(test_df[[\"text\"]])\n",
    "    \n",
    "    print(f\"Loaded {len(train_dataset)} training examples and {len(test_dataset)} test examples\")\n",
    "    return train_dataset, test_dataset\n",
    "\n",
    "train_dataset, test_dataset = load_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model and tokenizer...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c53b1c96de2d4ebab479a5896cfc8c45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Cell 5: Load Model and Tokenizer\n",
    "def load_model_and_tokenizer():\n",
    "    print(\"Loading model and tokenizer...\")\n",
    "    \n",
    "    # Configure quantization for memory efficiency\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.float16\n",
    "    )\n",
    "    \n",
    "    # Import and use login for authentication\n",
    "    from huggingface_hub import login\n",
    "    login(token=\"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\")  # Replace with your actual token\n",
    "    \n",
    "    # Load tokenizer with token authentication\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        model_id,\n",
    "        token=\"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\"  # Replace with your actual token\n",
    "    )\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    # Load model with quantization and token authentication\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_id,\n",
    "        quantization_config=bnb_config,\n",
    "        device_map=\"auto\",\n",
    "        token=\"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\",  # Replace with your actual token\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "    \n",
    "    print(\"Model and tokenizer loaded successfully\")\n",
    "    return model, tokenizer\n",
    "\n",
    "model, tokenizer = load_model_and_tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing data...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "453c0d34f0244cc4a23ba83f06f8203c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/152 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f29054f4db6e44c483db2cf31cde3196",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/17 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data preprocessing complete\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Cell 6: Preprocessing\n",
    "def preprocess_data(train_dataset, test_dataset, tokenizer):\n",
    "    print(\"Preprocessing data...\")\n",
    "    \n",
    "    def tokenize_function(examples):\n",
    "        return tokenizer(\n",
    "            examples[\"text\"],\n",
    "            truncation=True,\n",
    "            max_length=1024,  # Increased for Llama 3\n",
    "            padding=\"max_length\"\n",
    "        )\n",
    "    \n",
    "    # Tokenize datasets\n",
    "    tokenized_train = train_dataset.map(tokenize_function, batched=True)\n",
    "    tokenized_test = test_dataset.map(tokenize_function, batched=True)\n",
    "    \n",
    "    print(\"Data preprocessing complete\")\n",
    "    return tokenized_train, tokenized_test\n",
    "\n",
    "tokenized_train, tokenized_test = preprocess_data(train_dataset, test_dataset, tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying LoRA...\n",
      "Total parameters: 8043892736\n",
      "Trainable parameters: 13631488\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Cell 7: Apply LoRA\n",
    "def apply_lora(model):\n",
    "    print(\"Applying LoRA...\")\n",
    "    \n",
    "    # Prepare model for training\n",
    "    model = prepare_model_for_kbit_training(model)\n",
    "    \n",
    "    # Define LoRA configuration - adjusted for Llama 3\n",
    "    lora_config = LoraConfig(\n",
    "        r=16,                    # Rank - increased for Llama 3\n",
    "        lora_alpha=32,           # Alpha parameter \n",
    "        target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],  # Target more modules for Llama 3\n",
    "        lora_dropout=0.05,\n",
    "        bias=\"none\",\n",
    "        task_type=TaskType.CAUSAL_LM\n",
    "    )\n",
    "    \n",
    "    # Apply LoRA to model\n",
    "    model = get_peft_model(model, lora_config)\n",
    "    \n",
    "    # Print trainable parameters info\n",
    "    print(f\"Total parameters: {model.num_parameters()}\")\n",
    "    print(f\"Trainable parameters: {model.num_parameters(only_trainable=True)}\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "model = apply_lora(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up trainer...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Cell 8: Training Setup\n",
    "def setup_trainer(model, tokenized_train, tokenized_test, tokenizer):\n",
    "    print(\"Setting up trainer...\")\n",
    "    \n",
    "    # Training arguments - adjusted for Llama 3\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        num_train_epochs=10,              \n",
    "        per_device_train_batch_size=2,   # Reduced for larger model\n",
    "        per_device_eval_batch_size=2,    # Reduced for larger model\n",
    "        gradient_accumulation_steps=4,   # Added for larger model\n",
    "        warmup_steps=50,                # Increased for Llama 3\n",
    "        logging_steps=10,               \n",
    "        save_steps=50,                  \n",
    "        learning_rate=5e-5,              # Adjusted for Llama 3\n",
    "        weight_decay=0.01,               \n",
    "        fp16=True if torch.cuda.is_available() else False,                       \n",
    "        report_to=\"none\",\n",
    "        lr_scheduler_type=\"cosine\",      # Added for Llama 3\n",
    "        max_grad_norm=0.3,              # Added for stability\n",
    "    )\n",
    "    \n",
    "    # Data collator\n",
    "    data_collator = DataCollatorForLanguageModeling(\n",
    "        tokenizer=tokenizer,\n",
    "        mlm=False\n",
    "    )\n",
    "    \n",
    "    # Initialize Trainer\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_train,\n",
    "        eval_dataset=tokenized_test,\n",
    "        data_collator=data_collator,\n",
    "    )\n",
    "    \n",
    "    return trainer\n",
    "\n",
    "trainer = setup_trainer(model, tokenized_train, tokenized_test, tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='190' max='190' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [190/190 16:22, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2.913900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>2.812100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>2.355400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>1.697400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.285100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>1.097500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>1.038700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.952700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.896100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.830900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>0.747200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.723200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>0.676700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.635600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.604700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.577500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>0.573500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>0.539600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>0.560100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to ./llama3-lora-finetuned\n"
     ]
    }
   ],
   "source": [
    "# Cell 9: Training\n",
    "def train():\n",
    "    print(\"Starting training...\")\n",
    "    trainer.train()\n",
    "    \n",
    "    # Save the fine-tuned model\n",
    "    model.save_pretrained(output_dir)\n",
    "    tokenizer.save_pretrained(output_dir)\n",
    "    print(f\"Model saved to {output_dir}\")\n",
    "\n",
    "\n",
    "train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='9' max='9' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [9/9 00:03]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation results: {'eval_loss': 0.9683107733726501, 'eval_runtime': 3.6203, 'eval_samples_per_second': 4.696, 'eval_steps_per_second': 2.486, 'epoch': 10.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.9683107733726501,\n",
       " 'eval_runtime': 3.6203,\n",
       " 'eval_samples_per_second': 4.696,\n",
       " 'eval_steps_per_second': 2.486,\n",
       " 'epoch': 10.0}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cell 10: Evaluation\n",
    "def evaluate():\n",
    "    print(\"Evaluating model...\")\n",
    "    eval_results = trainer.evaluate()\n",
    "    print(f\"Evaluation results: {eval_results}\")\n",
    "    return eval_results\n",
    "\n",
    "evaluate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: evaluate in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (0.4.3)\n",
      "Requirement already satisfied: rouge-score in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (0.1.2)\n",
      "Requirement already satisfied: transformers in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (4.49.0)\n",
      "Requirement already satisfied: torch in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (2.2.1+cu121)\n",
      "Requirement already satisfied: pandas in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (2.1.4)\n",
      "Requirement already satisfied: numpy in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (1.26.4)\n",
      "Requirement already satisfied: datasets>=2.0.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from evaluate) (3.4.0)\n",
      "Requirement already satisfied: dill in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from evaluate) (0.3.8)\n",
      "Requirement already satisfied: requests>=2.19.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from evaluate) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from evaluate) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from evaluate) (3.5.0)\n",
      "Requirement already satisfied: multiprocess in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from evaluate) (0.70.16)\n",
      "Requirement already satisfied: fsspec>=2021.05.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from fsspec[http]>=2021.05.0->evaluate) (2024.12.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.7.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from evaluate) (0.29.3)\n",
      "Requirement already satisfied: packaging in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from evaluate) (24.2)\n",
      "Requirement already satisfied: absl-py in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from rouge-score) (2.1.0)\n",
      "Requirement already satisfied: nltk in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from rouge-score) (3.9.1)\n",
      "Requirement already satisfied: six>=1.14.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from rouge-score) (1.17.0)\n",
      "Requirement already satisfied: filelock in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers) (3.17.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: sympy in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch) (1.13.3)\n",
      "Requirement already satisfied: networkx in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch) (2.19.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: triton==2.2.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch) (2.2.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.8.93)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from pandas) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from pandas) (2025.1)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (19.0.1)\n",
      "Requirement already satisfied: aiohttp in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (3.11.13)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (2025.1.31)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: click in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from nltk->rouge-score) (8.1.8)\n",
      "Requirement already satisfied: joblib in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from nltk->rouge-score) (1.4.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (2.5.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.2)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (5.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (25.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (0.3.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.18.3)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33949c0bb7d647d8a8290df307dce4d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some parameters are on the meta device because they were offloaded to the cpu.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating responses for evaluation...\n",
      "Calculating ROUGE metrics...\n",
      "\n",
      "===== ROUGE Evaluation Results =====\n",
      "\n",
      "Individual Question Scores:\n",
      " question_id                                              question   rouge1   rouge2   rougeL\n",
      "           0 How should I handle a customer who wants to disput... 0.195652 0.044444 0.108696\n",
      "           1 What's the process for helping a new customer open... 0.202020 0.020619 0.101010\n",
      "           2 How do I handle a customer who is upset about over... 0.136364 0.023256 0.090909\n",
      "           3 What should I tell customers about our mobile bank... 0.269663 0.114943 0.247191\n",
      "           4 How should I respond when a customer asks for a lo... 0.193548 0.021978 0.107527\n",
      "\n",
      "Aggregate Scores:\n",
      "rouge1: 0.1994\n",
      "rouge2: 0.0448\n",
      "rougeL: 0.1311\n",
      "rougeLsum: 0.1311\n",
      "\n",
      "===== Sample Generated Responses =====\n",
      "\n",
      "Question 1: How should I handle a customer who wants to dispute a transaction?\n",
      "Generated: If the customer disputes a transaction, follow bank guidelines for investigation. Provide supporting documents and explain how transactions were proce...\n",
      "\n",
      "Question 2: What's the process for helping a new customer open a checking account?\n",
      "Generated: Customers need to provide valid ID, proof of address, and may require additional documentation depending on their banking history. The bank will asses...\n",
      "\n",
      "Question 3: How do I handle a customer who is upset about overdraft fees?\n",
      "Generated: Employees should empathize with customers, explain the fee structure, and offer alternative payment options if possible. Banks must comply with regula...\n",
      "\n",
      "Question 4: What should I tell customers about our mobile banking security features?\n",
      "Generated: Mobile banking uses advanced encryption technology, multi-factor authentication, and biometric verification to protect customer data. Customers must f...\n",
      "\n",
      "Question 5: How should I respond when a customer asks for a loan but has poor credit?\n",
      "Generated: Employees must assess the customerâ€™s financial situation and offer alternative options such as debt consolidation, secured loans with collateral, or m...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Install necessary packages\n",
    "!pip install evaluate rouge-score transformers torch pandas numpy\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import evaluate\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load your model\n",
    "model_path = \"./merged_model\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "# Define test cases with banking-specific scenarios\n",
    "test_cases = [\n",
    "    {\n",
    "        \"question\": \"How should I handle a customer who wants to dispute a transaction?\",\n",
    "        \"reference_answer\": \"First, listen carefully to the customer's concern. Ask for the transaction details including date, amount, and merchant. Explain the dispute process and timeframe. Document all details in the system. If the transaction is fraudulent, follow the fraud protocol and offer to cancel the card. If it's a merchant dispute, help the customer file a formal dispute form. Assure them you'll follow up within 5-7 business days with updates.\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What's the process for helping a new customer open a checking account?\",\n",
    "        \"reference_answer\": \"First, verify the customer's identity with two forms of ID. Explain our checking account options and their features. Once they select an account type, complete the application form in the system. Collect the initial deposit amount. Review all terms and conditions. Set up online banking access. Provide them with temporary checks and explain when their debit card and official checks will arrive. Inform them about our mobile app features.\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"How do I handle a customer who is upset about overdraft fees?\",\n",
    "        \"reference_answer\": \"Remain calm and professional. Listen actively to understand their specific concern. Review their account history to verify the overdraft charges. Explain our overdraft policy clearly. If this is their first occurrence, consider waiving the fee as a courtesy. Offer to set up overdraft protection services to prevent future fees. Document the interaction and any fee waivers in the customer's account notes.\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What should I tell customers about our mobile banking security features?\",\n",
    "        \"reference_answer\": \"Inform customers that our mobile banking app uses industry-leading encryption standards. Highlight the multi-factor authentication process that requires both password and biometric verification. Explain that we never store sensitive account information directly on their device. Mention our automatic timeout feature, transaction monitoring system, and instant fraud alerts. Emphasize that we provide zero liability protection for unauthorized transactions reported promptly.\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"How should I respond when a customer asks for a loan but has poor credit?\",\n",
    "        \"reference_answer\": \"Acknowledge their application respectfully without immediate rejection. Review their complete financial profile beyond just the credit score. Consider offering secured loan options or credit-builder products. Explain how our bank's financial education resources can help improve their credit over time. If we can't approve them now, provide specific reasons and suggestions for improvements. Offer to schedule a follow-up meeting in 3-6 months to reassess their situation.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# Function to generate responses from your model\n",
    "def generate_response(question, temperature=0.5):\n",
    "    prompt = f\"<|im_start|>user\\n{question}<|im_end|>\\n<|im_start|>assistant\\n\"\n",
    "    \n",
    "    # Ensure proper tokenization with attention mask\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True)\n",
    "    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            input_ids=inputs[\"input_ids\"],\n",
    "            attention_mask=inputs[\"attention_mask\"],\n",
    "            max_new_tokens=512,\n",
    "            temperature=temperature,\n",
    "            top_p=0.9,\n",
    "            repetition_penalty=1.2,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "    full_response = tokenizer.decode(outputs[0], skip_special_tokens=False)\n",
    "    try:\n",
    "        assistant_text = full_response.split(\"<|im_start|>assistant\\n\")[-1].split(\"<|im_end|>\")[0].strip()\n",
    "        return assistant_text\n",
    "    except:\n",
    "        # Fallback if token extraction fails\n",
    "        return tokenizer.decode(outputs[0][inputs[\"input_ids\"].shape[1]:], skip_special_tokens=True)\n",
    "\n",
    "# Generate model responses for all test cases\n",
    "print(\"Generating responses for evaluation...\")\n",
    "results = []\n",
    "for test_case in test_cases:\n",
    "    generated = generate_response(test_case[\"question\"])\n",
    "    results.append({\n",
    "        \"question\": test_case[\"question\"],\n",
    "        \"reference\": test_case[\"reference_answer\"],\n",
    "        \"generated\": generated\n",
    "    })\n",
    "\n",
    "# Convert to DataFrame for easier analysis\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Calculate ROUGE scores\n",
    "print(\"Calculating ROUGE metrics...\")\n",
    "rouge = evaluate.load('rouge')\n",
    "\n",
    "# Calculate ROUGE for each test case individually\n",
    "individual_scores = []\n",
    "for i, row in results_df.iterrows():\n",
    "    score = rouge.compute(\n",
    "        predictions=[row['generated']],\n",
    "        references=[row['reference']],\n",
    "        use_aggregator=True\n",
    "    )\n",
    "    individual_scores.append({\n",
    "        \"question_id\": i,\n",
    "        \"question\": row['question'][:50] + \"...\",\n",
    "        \"rouge1\": score['rouge1'],\n",
    "        \"rouge2\": score['rouge2'],\n",
    "        \"rougeL\": score['rougeL']\n",
    "    })\n",
    "\n",
    "# Calculate aggregate ROUGE scores\n",
    "aggregate_scores = rouge.compute(\n",
    "    predictions=results_df['generated'].tolist(),\n",
    "    references=results_df['reference'].tolist(),\n",
    "    use_aggregator=True\n",
    ")\n",
    "\n",
    "# Display results\n",
    "print(\"\\n===== ROUGE Evaluation Results =====\\n\")\n",
    "\n",
    "print(\"Individual Question Scores:\")\n",
    "individual_df = pd.DataFrame(individual_scores)\n",
    "print(individual_df.to_string(index=False))\n",
    "\n",
    "print(\"\\nAggregate Scores:\")\n",
    "for metric, value in aggregate_scores.items():\n",
    "    print(f\"{metric}: {value:.4f}\")\n",
    "\n",
    "print(\"\\n===== Sample Generated Responses =====\\n\")\n",
    "for i, row in results_df.iterrows():\n",
    "    print(f\"Question {i+1}: {row['question']}\")\n",
    "    print(f\"Generated: {row['generated'][:150]}...\\n\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
